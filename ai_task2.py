# -*- coding: utf-8 -*-
"""AI_task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15xYbN852Hc_xOW94saX6BHg86pF53cK7
"""

import pandas as pd
import numpy as np

from numpy import array
from sklearn.preprocessing import Normalizer

"""dataset"""

data={'X1':[30,40,50,20,15,60], 'X2':[40,50,20,15,60,70], 'X3':[50,20,15,60,70,50], 'Y':[20,15,60,70,50,40]}

dataset=pd.DataFrame(data)

dataset

"""task=create an artificial neural network that is capable of predicting the value of Y based on the values X1, X2, and X3.

ANN executes in two phases: feed forward and back propagation. in the feed forward phase we have one weight for each input layer. the weights of a neural network are the strings that we have to adjust in order to be able to correctly predict our output.

Defining both independent and dependent variables.
"""

#independent variables

input_set=np.array([[30,40,50],
                    [40,50,20],
                    [50,20,15],
                    [20,15,60],
                    [15,60,70],
                    [60,70,50]])#dependent variable

labels=np.array([[20,
                  15,
                  60,
                  70,
                  50,
                  40]])

labels=labels.reshape(6,1) #converts labels to vector

"""define hyperparameters"""

np.random.seed(42)

weights=np.random.rand(3,1)

bias=np.random.rand(1)

lr=0.05 #learning rate

"""define activation function and its derivative"""

def sigmoid(x):
  return 1/(1+np.exp(-x))

def sigmoid_derivative(x):
  return sigmoid(x)*(1-sigmoid(x))

"""train the model"""

for epoch in range(100):
  #FEED FORWARD PHASE
  inputs= input_set  #store the values from the input input_set to the inputs variable so that the value of input_set remains as it is in each iteration
  XW=np.dot(inputs, weights)+bias #dot product of of the input and weight and add bias to it.
  z=sigmoid(XW) #pass the dot product through the sigmoid activation function
  
  #BACK PROPAGATION PHASE
  error=z-labels #find the error
  print(error.sum())
  dcost=error #done using cost function to get the slope
  dpred=sigmoid_derivative(z)
  z_del=dcost*dpred
  inputs=input_set.T
  weights=weights-lr*np.dot(inputs,z_del)

  for num in z_del:
    bias=bias-lr*num #multiply the learning rate with the derivative to increase the speed of learning

"""make predictions"""

single_pt=np.array([1,0,0])
result=sigmoid(np.dot(single_pt, weights)+bias)
print(result)   #output here is classified as 1

single_pt=np.array([0,1,0])
result=sigmoid(np.dot(single_pt, weights)+bias)
print(result)   #output here is classified as 1